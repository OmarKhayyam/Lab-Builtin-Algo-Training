{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with Amazon SageMaker Linear Learner algorithm\n",
    "_**Single machine training for regression with Amazon SageMaker Linear Learner algorithm**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "   1. [Exploring the dataset](#Exploring-the-dataset)\n",
    "3. [Training the Linear Learner model](#Training-the-Linear-Learner-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Inference](#Inference)\n",
    "6. [Delete the Endpoint](#Delete-the-Endpoint)\n",
    "7. [Appendix](#Appendix)\n",
    "  1. [Downloading the dataset](#Downloading-the-dataset)\n",
    "  2. [libsvm to csv convertion](#libsvm-to-csv-convertion)\n",
    "  3. [Dividing the data](#Dividing-the-data)\n",
    "  4. [Data Ingestion](#Data-ingestion)\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMakerâ€™s implementation of the Linear Learner algorithm to train and host a regression model. We use the [Abalone data](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html) originally from the [UCI data repository](https://archive.ics.uci.edu/ml/datasets/abalone). \n",
    "\n",
    "The dataset contains 9 fields, starting with the Rings number which is a number indicating the age of the abalone (as age equals to number of rings plus 1.5). Usually the number of rings are counted through microscopes to estimate the abalone's age. So we will use our algorithm to predict the abalone age based on the other features which are mentioned respectively as below within the dataset. \n",
    "\n",
    "'Rings','sex','Length','Diameter','Height','Whole Weight','Shucked Weight','Viscera Weight' and 'Shell Weight'\n",
    "\n",
    "The above features starting from sex to Shell.weight are physical measurements that can be measured using the correct tools, so we improve the complixety of having to examine the abalone under microscopes to understand it's age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance with Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 buckets and prefixes that you want to use for training data and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "1. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m\n\u001b[1;32m      7\u001b[0m role \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mget_execution_role()\n\u001b[1;32m      8\u001b[0m region \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mSession()\u001b[38;5;241m.\u001b[39mregion_name\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for training data.\n",
    "# Feel free to specify a different bucket and prefix.\n",
    "data_bucket = f\"sagemaker-sample-files\"\n",
    "data_prefix = \"datasets/tabular/uci_abalone\"\n",
    "\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "output_prefix = \"sagemaker/DEMO-linear-learner-abalone-regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "We pre-processed the Abalone dataset [1] and stored in a S3 bucket. It was downloaded from the [National Taiwan University's CS department's tools for regression on the abalone dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/abalone). Scripts used in downloading and pre-processing can be found in the [Appendix](#Appendix). These include downloading data, converting data from libsvm format to csv format, dividing it into train, validation and test and uploading it to S3 bucket. \n",
    "\n",
    "The dataset contains a total of 9 fields. Throughout this notebook, they will be named as follows 'age','sex','Length','Diameter','Height','Whole.weight','Shucked.weight','Viscera.weight' and 'Shell.weight' respictively.\n",
    "\n",
    "the below data frame representation explain the value of each field.\n",
    "Note: the age field is in integer representation and the rest of the features are in the format of \"feature_number\":\"feature_value\"\n",
    "\n",
    "```\n",
    "**'data.frame'**:\n",
    "age              int  15 7 9 10 7 8 20 16 9 19 ...\n",
    "Sex               <feature_number>: Factor w/ 3 levels \"F\",\"I\",\"M\" values of 1,2,3\n",
    "Length            <feature_number>: float  0.455 0.35 0.53 0.44 0.33 0.425 ...\n",
    "Diameter          <feature_number>: float  0.365 0.265 0.42 0.365 0.255 0.3 ...\n",
    "Height            <feature_number>: float  0.095 0.09 0.135 0.125 0.08 0.095 ...\n",
    "Whole.weight      <feature_number>: float  0.514 0.226 0.677 0.516 0.205 ...\n",
    "Shucked.weight    <feature_number>: float  0.2245 0.0995 0.2565 0.2155 0.0895 ...\n",
    "Viscera.weight    <feature_number>: float  0.101 0.0485 0.1415 0.114 0.0395 ...\n",
    "Shell.weight      <feature_number>: float  0.15 0.07 0.21 0.155 0.055 0.12 ...\n",
    "```\n",
    ">[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "FILE_TRAIN = \"abalone_dataset1_train.csv\"\n",
    "FILE_TEST = \"abalone_dataset1_test.csv\"\n",
    "FILE_VALIDATION = \"abalone_dataset1_validation.csv\"\n",
    "\n",
    "# downloading the train, test, and validation files from data_bucket\n",
    "s3.download_file(data_bucket, f\"{data_prefix}/train_csv/{FILE_TRAIN}\", FILE_TRAIN)\n",
    "s3.download_file(data_bucket, f\"{data_prefix}/test_csv/{FILE_TEST}\", FILE_TEST)\n",
    "s3.download_file(data_bucket, f\"{data_prefix}/validation_csv/{FILE_VALIDATION}\", FILE_VALIDATION)\n",
    "s3.upload_file(FILE_TRAIN, output_bucket, f\"{output_prefix}/train/{FILE_TRAIN}\")\n",
    "s3.upload_file(FILE_TEST, output_bucket, f\"{output_prefix}/test/{FILE_TEST}\")\n",
    "s3.upload_file(FILE_VALIDATION, output_bucket, f\"{output_prefix}/validation/{FILE_VALIDATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Read in csv and store in a pandas dataframe\n",
    "\n",
    "df = pd.read_csv(\n",
    "    FILE_TRAIN,\n",
    "    sep=\",\",\n",
    "    encoding=\"latin1\",\n",
    "    names=[\n",
    "        \"age\",\n",
    "        \"sex\",\n",
    "        \"Length\",\n",
    "        \"Diameter\",\n",
    "        \"Height\",\n",
    "        \"Whole.weight\",\n",
    "        \"Shucked.weight\",\n",
    "        \"Viscera.weight\",\n",
    "        \"Shell.weight\",\n",
    "    ],\n",
    ")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "Let us prepare the handshake between our data channels and the algorithm. To do this, we need to create the `sagemaker.session.s3_input` objects from our [data channels](https://sagemaker.readthedocs.io/en/v1.2.4/session.html#). These objects are then put in a simple dictionary, which the algorithm consumes. Notice that here we use a `content_type` as `text/csv` for the pre-processed file in the data_bucket. We use two channels here one for training and the second one for validation. The testing samples from above will be used on the prediction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inputs for the fit() function with the training and validation location\n",
    "s3_train_data = f\"s3://{output_bucket}/{output_prefix}/train\"\n",
    "print(f\"training files will be taken from: {s3_train_data}\")\n",
    "s3_validation_data = f\"s3://{output_bucket}/{output_prefix}/validation\"\n",
    "print(f\"validation files will be taken from: {s3_validation_data}\")\n",
    "output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "print(f\"training artifacts output location: {output_location}\")\n",
    "\n",
    "#### START ####\n",
    "# generating the session.s3_input() format for fit() accepted by the sdk\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")\n",
    "####  END  ####\n",
    "\n",
    "# generating the session.s3_input() format for fit() accepted by the sdk\n",
    "#train_data = sagemaker.inputs.TrainingInput(\n",
    "#    s3_train_data,\n",
    "#    input_mode=input_mode,\n",
    "#    content_type=\"text/csv\",\n",
    "#    s3_data_type=\"S3Prefix\",\n",
    "#    record_wrapping=None,\n",
    "#    compression=None,\n",
    "#)\n",
    "#validation_data = sagemaker.inputs.TrainingInput(\n",
    "#    s3_validation_data,\n",
    "#    input_mode=input_mode,\n",
    "#    content_type=\"text/csv\",\n",
    "#    s3_data_type=\"S3Prefix\",\n",
    "#    record_wrapping=None,\n",
    "#    compression=None,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Linear Learner model\n",
    "\n",
    "First, we retrieve the image for the Linear Learner Algorithm according to the region.\n",
    "\n",
    "Then we create an [estimator from the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) using the Linear Learner container image and we setup the training parameters and hyperparameters configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the linear learner image according to the region\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve(\"linear-learner\", boto3.Session().region_name, version=\"1\")\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "job_name = \"DEMO-linear-learner-abalone-regression-\" + strftime(\"%Y%m%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "#### START ####\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    input_mode=\"File\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "linear.set_hyperparameters(\n",
    "    feature_dim=8,\n",
    "    epochs=16,\n",
    "    wd=0.01,\n",
    "    loss=\"absolute_loss\",\n",
    "    predictor_type=\"regressor\",\n",
    "    normalize_data=True,\n",
    "    optimizer=\"adam\",\n",
    "    mini_batch_size=100,\n",
    "    lr_scheduler_step=100,\n",
    "    lr_scheduler_factor=0.99,\n",
    "    lr_scheduler_minimum_lr=0.0001,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "####  END  ####\n",
    "\n",
    "#linear = sagemaker.estimator.Estimator(\n",
    "#    container,\n",
    "#    role,\n",
    "#    input_mode=input_mode,\n",
    "#    instance_count=1,\n",
    "#    instance_type=\"ml.m4.xlarge\",\n",
    "#    output_path=output_location,\n",
    "#    sagemaker_session=sess,\n",
    "#)\n",
    "\n",
    "#linear.set_hyperparameters(\n",
    "#    feature_dim=8,\n",
    "#    epochs=16,\n",
    "#    wd=0.01,\n",
    "#    loss=\"absolute_loss\",\n",
    "#    predictor_type=\"regressor\",\n",
    "#    normalize_data=True,\n",
    "#    optimizer=\"adam\",\n",
    "#    mini_batch_size=100,\n",
    "#    lr_scheduler_step=100,\n",
    "#    lr_scheduler_factor=0.99,\n",
    "#    lr_scheduler_minimum_lr=0.0001,\n",
    "#    learning_rate=0.1,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "After configuring the Estimator object and setting the hyperparameters for this object. The only remaining thing to do is to train the algorithm. The following cell will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that we requested while creating the Estimator classes are provisioned and are setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data. Therefore it might be a few minutes before we start getting data logs for our training jobs. The data logs will also print out Mean Average Precision (mAP) on the validation data, among other losses, for every run of the dataset once or one epoch. This metric is a proxy for the quality of the algorithm.\n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as output_path in the estimator. For this example,the training time takes between 4 and 6 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linear.fit(inputs={\"train\": train_data, \"validation\": validation_data}, job_name=job_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
